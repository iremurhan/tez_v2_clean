# Cross-Modal Retrieval Configuration (Tez v2 Clean)

# ==========================================
# Data Configuration
# ==========================================
data:
  images_path: "datasets/coco"
  # Caption path: Using Karpathy JSON structure usually
  captions_path: "datasets/coco/caption_datasets/dataset_coco.json"
  batch_size: 128
  num_workers: 8
  max_length: 77  # Max token length for CLIP (CLIP's default context length)

# ==========================================
# Model Architecture
# ==========================================
model:
  # CLIP Model: Pre-trained on 400M image-text pairs
  image_model_name: "openai/clip-vit-large-patch14-336"  # CLIP ViT-L/14 @ 336px
  embed_dim: null        # Final embedding space (null = use CLIP's native dimension, or specify custom dim)
  dropout: 0.1           # Dropout for additional projection heads
  
  # Unfreezing Configuration (ViT-L/14-336 has 24 blocks: 0-23)
  unfreeze_vision_layers: 3   # Unfreeze last N vision transformer blocks (0 = frozen)
  unfreeze_strategy: "full"   # Options: "full", "attention", "mlp", "layernorm", "bias"

# ==========================================
# Loss Configuration (Inter + Intra)
# ==========================================
loss:
  use_clip_loss: true    # Use CLIP's native loss with LEARNABLE temperature (recommended)
  temperature: 0.07      # Only used if use_clip_loss: false
  intra_img_weight: 0    # Only used if use_clip_loss: false
  intra_txt_weight: 0    # Only used if use_clip_loss: false

# ==========================================
# Training Configuration
# ==========================================
training:
  epochs: 50

  # --- Learning Rate Groups ---
  # 1. CLIP's projection layers (visual_projection, text_projection)
  clip_projection_lr: 0.00005  # 5e-5 (increased from 1e-5)
  
  # 2. Custom MLP heads (image_proj, text_proj) - only used if embed_dim != CLIP projection_dim
  head_lr: 0.0005              # 5e-4
  
  # 3. Backbone (unfrozen vision transformer blocks) - only used if unfreeze_vision_layers > 0
  backbone_lr: 0.000002        # 2e-6 (very small to prevent catastrophic forgetting)
  
  seed: 42
  optimizer: "adamw"           # AdamW is better for Transformers
  weight_decay: 0.01           # Higher weight decay for CLIP
  scheduler: "cosine"
  warmup_epochs: 2

# ==========================================
# Logging Configuration
# ==========================================
logging:
  use_wandb: true
  wandb_project: "coco-retrieval-thesis"
  log_freq: 500              # Log every 500 batches to see progress
  eval_freq: 1              # Evaluate every 1 epochs
  save_freq: 3              # Save checkpoint every 3 epochs
  checkpoint_dir: "checkpoints"

# ==========================================
# Augmentation Configuration
# ==========================================
augment:
  image:
      enabled: true

# ==========================================
# Debug Configuration
# ==========================================
debug:
  debug_mode: true         # Set true to use tiny subset
  debug_samples: 100       # Only use 100 images for debugging
  disable_wandb_sync: true  # Don't upload debug logs