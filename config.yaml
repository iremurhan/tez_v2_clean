# Cross-Modal Retrieval Configuration (Tez v2 Clean)

# ==========================================
# Data Configuration
# ==========================================
data:
  images_path: "datasets/coco"
  # Caption path: Using Karpathy JSON structure usually
  captions_path: "datasets/coco/caption_datasets/dataset_coco.json"
  batch_size: 64
  num_workers: 8
  max_length: 77  # Max token length for CLIP (CLIP's default context length)

# ==========================================
# Model Architecture
# ==========================================
model:
  # CLIP Model: Pre-trained on 400M image-text pairs
  image_model_name: "openai/clip-vit-base-patch32"  # CLIP ViT-B/32
  # Note: text_model_name is ignored when using CLIP (CLIP has its own text encoder)
  text_model_name: "openai/clip-vit-base-patch32"   # Kept for compatibility
  image_dim: 512         # CLIP projection dimension (reference only)
  text_dim: 512          # CLIP projection dimension (reference only)
  embed_dim: 256         # Final embedding space (additional projection)
  dropout: 0.1           # Dropout for additional projection heads

# ==========================================
# Loss Configuration (Inter + Intra)
# ==========================================
loss:
  temperature: 0.07      # For Contrastive Loss (InfoNCE)
  intra_img_weight: 1  # Weight for Intra-modal structure loss (Image-to-Image)
  intra_txt_weight: 1  # Weight for Intra-modal structure loss (Text-to-Text)

# ==========================================
# Training Configuration
# ==========================================
training:
  epochs: 20
  # CLIP Fine-tuning Learning Rates
  clip_projection_lr: 0.00001  # 1e-5 (CLIP's projection layers)
  head_lr: 0.0005              # 5e-4 (Custom projection heads)
  seed: 42
  optimizer: "adamw"           # AdamW is better for Transformers
  weight_decay: 0.01           # Higher weight decay for CLIP
  scheduler: "cosine"
  warmup_epochs: 2

# ==========================================
# Logging Configuration
# ==========================================
logging:
  use_wandb: true
  wandb_project: "coco-retrieval-thesis"
  log_freq: 500              # Log every 500 batches to see progress
  eval_freq: 1              # Evaluate every 1 epochs
  save_freq: 3              # Save checkpoint every 3 epochs
  checkpoint_dir: "checkpoints"

# ==========================================
# Augmentation Configuration
# ==========================================
augment:
  image:
      enabled: true

# ==========================================
# Debug Configuration
# ==========================================
debug:
  debug_mode: true         # Set true to use tiny subset
  debug_samples: 100       # Only use 100 images for debugging
  disable_wandb_sync: true  # Don't upload debug logs