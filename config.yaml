# Cross-Modal Retrieval Configuration (Tez v2 Clean)

# ==========================================
# Data Configuration
# ==========================================
data:
  features_path: "datasets/coco/features_karpathy"
  # Caption path: Using Karpathy JSON structure usually
  captions_path: "datasets/coco/caption_datasets/dataset_coco.json"
  batch_size: 256
  num_workers: 4
  max_length: 50  # Max token length for DistilBERT (saves compute)

# ==========================================
# Model Architecture
# ==========================================
model:
  image_dim: 2048        # ResNet feature dimension (Pool5)
  text_model_name: "distilbert-base-uncased" # HuggingFace model ID
  text_dim: 768          # DistilBERT output dimension
  embed_dim: 256         # Joint embedding space (Start small for speed)
  dropout: 0.1           # Dropout probability for projection heads

# ==========================================
# Loss Configuration (Inter + Intra)
# ==========================================
loss:
  temperature: 0.07      # For Contrastive Loss (InfoNCE)
  intra_img_weight: 0.1  # Weight for Intra-modal structure loss (Image-to-Image)
  intra_txt_weight: 0.0  # Weight for Intra-modal structure loss (Text-to-Text)

# ==========================================
# Training Configuration
# ==========================================
training:
  epochs: 40
  # CRITICAL: Separated learning rates to protect pre-trained BERT
  text_encoder_lr: 0.00002  # 2e-5 (Very small for BERT)
  head_lr: 0.001            # 1e-3 (Standard for Projection Heads)
  seed: 42
  optimizer: "adamw"        # AdamW is better for Transformers
  weight_decay: 0.0001
  scheduler: "cosine"
  warmup_epochs: 1          # 1 epoch warmup is sufficient

# ==========================================
# Logging Configuration
# ==========================================
logging:
  use_wandb: true
  wandb_project: "coco-retrieval-thesis"
  log_freq: 500              # Log every 500 batches to see progress
  eval_freq: 2              # Evaluate every 2 epochs
  save_freq: 5              # Save checkpoint every 5 epochs
  checkpoint_dir: "checkpoints"

# ==========================================
# Augmentation Configuration
# ==========================================
augment:
  image:
    feature_jitter:
      enabled: true
      std: 0.02

# ==========================================
# Debug Configuration
# ==========================================
debug:
  debug_mode: true         # Set true to use tiny subset
  debug_samples: 100       # Only use 100 images for debugging
  disable_wandb_sync: true  # Don't upload debug logs